{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGGmsR5vpTTJ7A/qsV73Vf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyozeGS/IABD/blob/main/7RO/T5/7RO_3_en_raya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTRENANDO A UN AGENTE PARA JUGAR AL TRES EN RAYA MEDIANTE Q-LEARNING\n",
        "\n",
        "Se implementará el algoritmo Q-Learning para que un agente aprenda a jugar al tres en raya.\n",
        "\n",
        "Se evaluará el desempeño del agente analizando el porcentaje de partidas ganadas frente a jugador aleatorio y experto.\n",
        "\n",
        "Enlace al cuaderno:\n",
        "\n",
        "https://colab.research.google.com/drive/1I7VZcHsz2qYdlCaKmjscKRjEqP1SmdVo#scrollTo=HqMl35xfE547"
      ],
      "metadata": {
        "id": "HqMl35xfE547"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planificación"
      ],
      "metadata": {
        "id": "ekepSPaZpc5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Librerías\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "SNl5gyBsDBxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación"
      ],
      "metadata": {
        "id": "2Q4SFp17ND_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tableros válidos y matriz de recompensas\n",
        "\n",
        "Se comprueban todas las combinaciones de 0,1,2 en nueve casillas, representando los tableros posibles y los válidos una vez eliminados aquellas con más 2 que 1 o con dos ó más 1 que 2.\n",
        "\n",
        "Además se crea un vector con las recompensas de victoria/derrota/empate para cada combinación válida y según el resultado.\n",
        "\n",
        "Cada fila representaría una tablero posible, recorrido de izquierda a derecha y de arriba hacia abajo, con 'x' como 2, 'o' como 1 y los restantes como 0. Un ejemplo sería:\n",
        "\n",
        "|   |   |   |\n",
        "|---|---|---|\n",
        "| X |   | O  |\n",
        "| O | X | X |\n",
        "| X |   | O |\n",
        "\n",
        "[1, 0, 2, 2, 1, 1, 1, 0, 2]\n",
        "\n"
      ],
      "metadata": {
        "id": "tLtIUfHaNLom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectores de tableros válidos y matriz de recompensas\n",
        "def create_board_and_rewards_vectors(rewards, info=False):\n",
        "\n",
        "  #Matriz de tableros válidos y array de victoria\n",
        "  board_states = []\n",
        "  v_wins = np.ones(3)\n",
        "\n",
        "  #Variables de recompensas\n",
        "  R = []\n",
        "  r_win = rewards[0]\n",
        "  r_lose = rewards[1]\n",
        "  r_draw = rewards[2]\n",
        "\n",
        "  #variables auxiliares para información\n",
        "  count = 0\n",
        "  valid_count = 0\n",
        "  winner_counts = [0,0,0] #Win, Lose, Draw\n",
        "\n",
        "  #Combinatoria de 3 (0,1 y 2) posibles valores en 9 casillas\n",
        "  for a in range(3):\n",
        "    for b in range(3):\n",
        "      for c in range(3):\n",
        "        for d in range(3):\n",
        "          for e in range(3):\n",
        "            for f in range(3):\n",
        "              for g in range(3):\n",
        "                for h in range(3):\n",
        "                  for i in range(3):\n",
        "\n",
        "                    count +=1 #contador de combinaciones\n",
        "                    board_state = [a, b, c, d, e, f, g, h, i] #vector representación del tablero\n",
        "\n",
        "                    #Se omite el tablero si J1 no tiene igual cantidad de fichas que J2 o una más.\n",
        "                    diff_pieces = board_state.count(1) - board_state.count(2)\n",
        "                    if diff_pieces > 1 or diff_pieces < 0:\n",
        "                      continue\n",
        "\n",
        "                    #Se detectan 3 en raya de ambos jugadores en el tablero\n",
        "                    players_win = [False, False]\n",
        "                    m = np.asmatrix(np.array(board_state).reshape(3,3))\n",
        "                    #Diagonales\n",
        "                    diag1 = m.diagonal()\n",
        "                    diag2 = np.fliplr(m).diagonal()\n",
        "\n",
        "                    for i in range(1,3):\n",
        "                      #Jugador i gana en filas o columnas\n",
        "                      if np.any(np.all(i*v_wins == m, axis=0)) or np.any(np.all(i*v_wins == m, axis=1)):\n",
        "                        players_win[i-1] = True\n",
        "                      #Jugador i gana en diagonales\n",
        "                      if np.all(i*v_wins == diag1) or np.all(i*v_wins == diag2):\n",
        "                        players_win[i-1] = True\n",
        "                    #Si ambos han ganado se omite el tablero\n",
        "                    if all(players_win):\n",
        "                      continue\n",
        "                    #Se añade el tablero al array de tableros válidos\n",
        "                    board_states.append(board_state)\n",
        "\n",
        "                    #Se añade la recompensa del tablero al vector de recompensas\n",
        "                    if players_win[0]:\n",
        "                      R.append(r_win)\n",
        "                      winner_counts[0] += 1\n",
        "                    elif players_win[1]:\n",
        "                      R.append(r_lose)\n",
        "                      winner_counts[1] += 1\n",
        "                    elif board_state.count(0) == 0: #Si el tablero está lleno y no se hay ganador\n",
        "                      R.append(r_draw)\n",
        "                      winner_counts[2] += 1\n",
        "                    else:\n",
        "                      R.append(0)\n",
        "\n",
        "                    valid_count += 1 #contador de estados\n",
        "  #INFO\n",
        "  if info:\n",
        "    print(f\"{valid_count} Estados Validos de {count} combinaciones\")\n",
        "    print(f\"Estados Finales: {winner_counts[0]} V / {winner_counts[1]} D / {winner_counts[2]} E\")\n",
        "\n",
        "  return board_states, R\n",
        "\n",
        "## LLamada de la función\n",
        "# Recompensas de victoria/derrota/empate\n",
        "_rewards = [10, -10, 1]\n",
        "_board_states, _R = create_board_and_rewards_vectors(_rewards, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_xwPQgrzGZ7",
        "outputId": "bd9ed105-9e52-4111-c4bd-81a5e9ac64a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5890 Estados Validos de 19683 combinaciones\n",
            "Estados Finales: 942 V / 412 D / 16 E\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "winner_counts = [0,0,0] #Win, Lose, Draw\n",
        "winner_counts[1] += 1\n",
        "winner_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FKkvoIP9tnD",
        "outputId": "219517f8-608b-4133-bca3-3d3dc9def9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diccionarios\n",
        "\n",
        "Diccionarios directo e inverso con valores (representación numérica del tablero) y estados(índices) de cada tablero\n",
        "\n",
        "el valor se cálcula de cada tablero tal que:\n",
        "\n",
        "$$ v = a \\cdot 3^8 + b \\cdot 3^7 + c \\cdot 3^6 + d \\cdot 3^5 + e \\cdot 3^4 + f \\cdot 3^3 + g \\cdot 3^2 + h \\cdot 3^1 + i \\cdot 3^0 $$\n",
        "\n",
        "siendo los coeficientes en el rango a-i los valores del array para el tablero en orden recorrido."
      ],
      "metadata": {
        "id": "vbLDDm33NRGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dicts_state_value(board_states, info=False):\n",
        "  #Diccionarios\n",
        "  dict_state_value = {}\n",
        "  dict_value_state = {}\n",
        "\n",
        "  #Se rellenan ambos diccionarios\n",
        "  for index, board_state in enumerate(board_states):\n",
        "    board_value = sum([board_state[i] * 3**(8-i) for i in range(9)])\n",
        "    dict_value_state[board_value] = index\n",
        "    dict_state_value[index] = board_value\n",
        "\n",
        "  #INFO\n",
        "  if info:\n",
        "    print(\"E -> V / V -> E\")\n",
        "    for i in range(10):\n",
        "      print([*dict_state_value.keys()][i], \"->\", [*dict_state_value.values()][i], \"/\",\n",
        "            [*dict_value_state.keys()][i], \"->\", [*dict_value_state.values()][i])\n",
        "\n",
        "  return dict_state_value, dict_value_state\n",
        "\n",
        "## LLamada de la función\n",
        "_dict_state_value, _dict_value_state = create_dicts_state_value(_board_states, True)"
      ],
      "metadata": {
        "id": "Zcyj7l3q7mWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac10745-634b-47df-8b9d-fc78e26c12e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E -> V / V -> E\n",
            "0 -> 0 / 0 -> 0\n",
            "1 -> 1 / 1 -> 1\n",
            "2 -> 3 / 3 -> 2\n",
            "3 -> 5 / 5 -> 3\n",
            "4 -> 7 / 7 -> 4\n",
            "5 -> 9 / 9 -> 5\n",
            "6 -> 11 / 11 -> 6\n",
            "7 -> 14 / 14 -> 7\n",
            "8 -> 15 / 15 -> 8\n",
            "9 -> 16 / 16 -> 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matriz de Transición\n",
        "\n",
        "Matriz que para cada estado indica los estados a los que se transitaría con cada acción realizada. En casa de ser una acción no válida se añadiría menos uno.\n",
        "\n",
        "Por ejemplo, partiendo del estado 1701, cuyo tablero es:\n",
        "\n",
        "|   |   |   |\n",
        "|---|---|---|\n",
        "|   | O |   |\n",
        "| O | O | X |\n",
        "| X | X |   |\n",
        "\n",
        "1701 => [0, 2, 0, 2, 2, 1, 1, 1, 0] => V 5061\n",
        "\n",
        "La matriz de transición en dicha fila pondría las posiciones ocupadas a -1.\n",
        "\n",
        "T[1701] -> [-, -1, -, -1, -1, -1, -1, -1, -]\n",
        "\n",
        "Y para las tres posiciones restante calcularía que el valor actual del tablero es 5061 (usando la fóruma vista antes) y le aplicaría el valor del nuevo movimiento. Por ejemplo en la última casilla sería 1 * 3^0 = 1, quedando el tablero a 5062. Con dicho valor se busca en el diccionario inverso que corresponde al estado 1702 y ese valor se añade a la matriz de transicción para esa fila y en la columna designada para la acción 8 (mover en la última casilla).\n",
        "\n",
        "|   |   |   |\n",
        "|---|---|---|\n",
        "|   | O |   |\n",
        "| O | O | X |\n",
        "| X | X | X |\n",
        "\n",
        "[0, 2, 0, 2, 2, 1, 1, 1, 1] => V 5062 => 1702\n",
        "\n",
        "De ese modo, si hacemos el cálculo para los otros dos movimientos posibles en las casillas superiores la matriz de transición en esa fila quedaría de la forma\n",
        "\n",
        "[3685,   -1, 1960,   -1,   -1,   -1,   -1,   -1, 1702]"
      ],
      "metadata": {
        "id": "acFAeDlGNXLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transition_matrix(board_states, R, dict_state_value, dict_value_state, info=False):\n",
        "  # Matriz de Transición\n",
        "  T = []\n",
        "  for index, board_state in enumerate(board_states):\n",
        "\n",
        "    # Si el estado es un estado final se añade array de nueve -1\n",
        "    if R[index] != 0:\n",
        "      T.append([-1]*9)\n",
        "      continue\n",
        "\n",
        "    # Si juega J1(igualdad de fichas) se añade un 1  y si juega J2 se añade un 2\n",
        "    piece = 1 if board_state.count(1) == board_state.count(2) else 2\n",
        "\n",
        "    # Se crea una nueva fila a añadir a la matriz de transiciones\n",
        "    T_row = []\n",
        "    for position in range(9):\n",
        "      # Se calcula estado siguiente en cada posicion de añadir 1/2 a la casilla\n",
        "      if board_state[position] == 0:\n",
        "        T_row.append(dict_value_state[dict_state_value[index] + piece*3**(8-position)])\n",
        "      # Se añade -1 a la transición si ya había un 1/2 en la casilla\n",
        "      else:\n",
        "        T_row.append(-1)\n",
        "    T.append(T_row)\n",
        "  T = np.array(T, dtype=int)\n",
        "\n",
        "  # INFO\n",
        "  if info:\n",
        "    print(\"Primeras filas de la matriz T:\\n\", T[0:5])\n",
        "\n",
        "  return T\n",
        "\n",
        "# LLamada de la función\n",
        "_T = create_transition_matrix(_board_states, _R, _dict_state_value, _dict_value_state, True)"
      ],
      "metadata": {
        "id": "nitWVzVs38ZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4f8ddd-a27c-4bab-9b10-b546b48a088b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras filas de la matriz T:\n",
            " [[2103  746  265   96   35   13    5    2    1]\n",
            " [4154 1487  530  191   70   26   10    4   -1]\n",
            " [4155 1488  531  192   71   27   11   -1    3]\n",
            " [2105  748  267   98   37   15    7   -1   -1]\n",
            " [2107  750  269  100   39   17    9   -1   -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "RDm4jhJMNekG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones Auxiliares"
      ],
      "metadata": {
        "id": "kEN3kM0p4VWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función que obtiene los índices de la matriz de transición para un estado donde el valor no es -1, que corresponderían a jugadas válidas, y devuelve uno al azar."
      ],
      "metadata": {
        "id": "Gj7tYvHoGx7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_random_action(state, T):\n",
        "\n",
        "  indexes = np.where(T[state] != -1)[0]\n",
        "  try:\n",
        "    return np.random.choice(indexes)\n",
        "  except ValueError:\n",
        "    print(f\"Warning: 'choose_random_action'! No hay transiciones válidas para el estado {state}. 'None' devuelto.\")\n",
        "\n",
        "#Examples\n",
        "T_example=np.zeros((4,9),dtype=float)\n",
        "for i in range(T_example.shape[0]):\n",
        "  for j in range(T_example.shape[1]):\n",
        "    T_example[i,j] = np.random.randint(10)\n",
        "  if i == 2:\n",
        "    T_example[i] = [-1]*T_example.shape[1]\n",
        "\n",
        "print(\"Ejemplo 1:\")\n",
        "print(\"T:\",T_example[2])\n",
        "print(choose_random_action(2, T_example))\n",
        "print(\"Ejemplo 2:\")\n",
        "print(\"T:\",_T[0])\n",
        "for i in range(5):\n",
        "  print(choose_random_action(0, T_example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSTwcFHe2GzG",
        "outputId": "fcb4fb04-4c37-4d8d-cfb3-c70266cce3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo 1:\n",
            "T: [-1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "Warning: 'choose_random_action'! No hay transiciones válidas para el estado 2. 'None' devuelto.\n",
            "None\n",
            "Ejemplo 2:\n",
            "T: [2103  746  265   96   35   13    5    2    1]\n",
            "0\n",
            "2\n",
            "0\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función que calcula el máximo valor de una fila de la matriz Q, ignorando aquellos valores que no sean válidos (-1) según la matriz T para el mismo estado."
      ],
      "metadata": {
        "id": "NaYAw7pLH39-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_q_max(state, T, Q, ignore_invalid_state=False):\n",
        "\n",
        "  T_mask = T[state]==-1\n",
        "  if T_mask.all() and ignore_invalid_state:\n",
        "    return 0\n",
        "  else:\n",
        "    return np.ma.max(np.ma.masked_array(Q[state], T_mask))"
      ],
      "metadata": {
        "id": "0i-qoOpJDxOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función que obtiene los índices de la matriz de transición donde el valor es igual al máximo de la matriz Q para un estado dado."
      ],
      "metadata": {
        "id": "yBTIiSm5HZfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_best_action(state, T, Q):\n",
        "\n",
        "  q_value = calculate_q_max(state, T, Q, False)\n",
        "  indexes = np.where(Q[state,] == q_value)[0]\n",
        "  try:\n",
        "    return np.random.choice(indexes)\n",
        "  except ValueError:\n",
        "    print(f\"Warning: 'choose_best_action'! No hay transiciones válidas para el estado {state}. 'None' devuelto.\")\n",
        "\n",
        "#Examples\n",
        "T_example=np.zeros((4,9),dtype=float)\n",
        "Q_example=np.zeros((4,9),dtype=float)\n",
        "for i in range(T_example.shape[0]):\n",
        "  for j in range(T_example.shape[1]):\n",
        "    T_example[i,j] = np.random.randint(10)\n",
        "    T_example[i,j] = np.random.choice([T_example[i,j], -1])\n",
        "    Q_example[i,j] = np.random.random()\n",
        "  if i == 2:\n",
        "    T_example[i] = [-1]*T_example.shape[1]\n",
        "\n",
        "print(\"Ejemplo 1:\")\n",
        "print(\"T:\",T_example[2])\n",
        "print(\"Q:\",Q_example[2])\n",
        "print(choose_best_action(2, T_example, Q_example))\n",
        "print(\"Ejemplo 2:\")\n",
        "print(\"T:\",T_example[0])\n",
        "print(\"Q:\",Q_example[0])\n",
        "for i in range(5):\n",
        "  print(choose_best_action(0, T_example, Q_example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivx64eT9A73M",
        "outputId": "1540ee13-93b4-4841-92e0-2fe8dddd7169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo 1:\n",
            "T: [-1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "Q: [0.6084536  0.94259351 0.58737874 0.06440216 0.23095189 0.16038692\n",
            " 0.44924275 0.20853705 0.49841264]\n",
            "Warning: 'choose_best_action'! No hay transiciones válidas para el estado 2. 'None' devuelto.\n",
            "None\n",
            "Ejemplo 2:\n",
            "T: [-1. -1.  2.  3. -1. -1. -1. -1. -1.]\n",
            "Q: [0.88008524 0.00433188 0.57100072 0.64769138 0.54292327 0.14334511\n",
            " 0.78969894 0.33311566 0.00524252]\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jugador que conoce la mayoría de movimientos iniciales pero mantiene algunas aperturas en segundas/terceras jugadas."
      ],
      "metadata": {
        "id": "Sxv5l7e9gWDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_advanced_action(state, T, R, board_states):\n",
        "\n",
        "  board_state = board_states[state]\n",
        "\n",
        "  #Comprueba victoria o empate directo\n",
        "  for a in range(T.shape[1]):\n",
        "    if T[state,a] != -1:\n",
        "      if R[T[state,a]] != 0 or board_state.count(0) == 1:\n",
        "        return a\n",
        "\n",
        "  #Juega al medio\n",
        "  if board_state[4] == 0:\n",
        "    return 4\n",
        "\n",
        "  #Juega a la esquina en segunda jugada\n",
        "  if board_state == [0,0,0,0,1,0,0,0,0]:\n",
        "    return np.random.choice([0,2,6,8])\n",
        "\n",
        "  #Comprueba peligro si el otro jugador tiene 2 fichas en fila y bloqueo\n",
        "  m = np.asmatrix(np.array(board_state).reshape(3,3))\n",
        "\n",
        "  # peligros horizontales y verticales\n",
        "  for i in range(3):\n",
        "    for j in range(3):\n",
        "      # 2 fichas iguales en horizontal\n",
        "      if np.count_nonzero(m[i]) == 2 and  m[i].sum() %2 == 0 and m[i,j] == 0:\n",
        "\n",
        "        return j + i*3\n",
        "      # 2 fichas iguales en vertical\n",
        "      if np.count_nonzero(m.T[j]) == 2 and m.T[j].sum() %2 == 0 and m[i,j] == 0:\n",
        "        return j + i*3\n",
        "\n",
        "  # peligros diagonales\n",
        "  diag1 = m.diagonal()\n",
        "  diag2 = np.fliplr(m).diagonal()\n",
        "  for i in range(3):\n",
        "      if np.count_nonzero(diag1) == 2 and diag1.sum() %2 == 0 and m[i,i] == 0:\n",
        "        return i*3 + i\n",
        "      if np.count_nonzero(diag2) == 2 and diag2.sum() %2 == 0 and m[i,2-i] == 0:\n",
        "        return i*3 + 2-i\n",
        "\n",
        "  #condiciones especiales\n",
        "  #Diagonal 121 -> mover lateral\n",
        "  if board_state == [1,0,0,0,2,0,0,0,1] or board_state == [0,0,1,0,2,0,1,0,0]:\n",
        "    return np.random.choice([1,3,5,7])\n",
        "  #Diagonal 112 o simetricas -> mover igual column o fila\n",
        "  if m.sum() == 4 and (diag1.sum() == 4 or diag2.sum() == 4):\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "        if m[i,j] == 2:\n",
        "          return np.random.choice([(2-i)*3 + j, 3*i + (2-j)])\n",
        "\n",
        "  #aleatorio\n",
        "  empty_positions = []\n",
        "  for i in range(T.shape[1]):\n",
        "    if board_state[i] == 0:\n",
        "      empty_positions.append(i)\n",
        "  return np.random.choice(empty_positions)"
      ],
      "metadata": {
        "id": "BXRI7IPxL6so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matriz Q\n",
        "\n",
        "Existen 3 versiones del algoritmo:\n",
        "\n",
        "- Algoritmo simple que entrena J1 usando turnos del rival\n",
        "- Algoritmo que entrena J1 saltando los turno del rival\n",
        "- Algoritmo que entre J1 y J2 saltando turnos del  rival"
      ],
      "metadata": {
        "id": "bxQoxIpG4eI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmo que entrena la matriz Q a raíz de la ecuación de Bellman. El entrenamiento es sólo para el jugador 1 y las estados en los que mueve el jugador 2 también se retoralimentan de las recompensas, lo implicaría que la IA como jugador 2 ayudaría a ganar al 1.\n",
        "\n",
        "El factor epsilon-greedy se ha dejado como ecuación de segundo grado para que el 70% prevalezca la fase de exploración y en el último 30% la fase de explotación vaya teniendo más incidencia."
      ],
      "metadata": {
        "id": "T8b35fAIqTP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_quality_matriz_simple(T, R, board_states, episodes, v, y, info=False):\n",
        "  # Matriz de calidad\n",
        "  Q = np.zeros((T.shape[0],T.shape[1]),dtype=float)\n",
        "\n",
        "  s = 0 # Estado inicial\n",
        "  episode = 0 # Inicialización del entrenamiento\n",
        "\n",
        "  while(episode < episodes):\n",
        "\n",
        "    # Epsilon-Greedy: Selección de acción\n",
        "    epsilon = 1 - (episode/episodes)**2\n",
        "\n",
        "    # Acción aleatoria\n",
        "    if np.random.rand() < epsilon:\n",
        "      a=choose_random_action(s, T)\n",
        "    # Acción según la matriz de calidad (Q)\n",
        "    else:\n",
        "      a=choose_best_action(s, T, Q)\n",
        "\n",
        "    next_s = T[s,a]\n",
        "\n",
        "    # Actualización de la matriz de calidad\n",
        "    Qmax = calculate_q_max(next_s, T, Q, True)\n",
        "    Q[s,a] = (1-v) * Q[s,a] + v * (R[next_s] + y*Qmax)\n",
        "\n",
        "    # Cambio de estado\n",
        "    # La segunda condición equivale a completar el tablero si no hubiese recompensas para dicho caso. Ej: r_draw = 0\n",
        "    if (R[next_s] != 0) or (board_states[next_s].count(0) == 0):\n",
        "      s = 0\n",
        "    else:\n",
        "      s = next_s\n",
        "    episode += 1\n",
        "\n",
        "  # INFO\n",
        "  if info:\n",
        "    # Comprobación de los estados que no han actualizado su valor en la matriz de calidad y no son finales con recompensa\n",
        "    Rn0 = len(np.array(R)[np.array(R)!=0])\n",
        "    print(\"Estados con recompensas =>\", Rn0)\n",
        "    Qa0 = Q[(Q==0).all(axis=1)].shape[0]\n",
        "    print(\"Filas de Q sin entrenar =>\", Qa0)\n",
        "    #Combiene volver a entrenar si algún estado no ha sido entrenado\n",
        "    print(\"Índices no entrenados =>\", np.setxor1d(np.where((Q == 0).all(axis=1))[0], np.where(np.array(R)!=0)[0]))\n",
        "\n",
        "  return Q\n",
        "\n",
        "_episodes = 400000 # Nº de actualizaciones de la matriz Q\n",
        "_learning_rate = 0.1 # Factor de aprendizaje menor a 1/9 que de mayor peso a la experiencia\n",
        "_discount_rate = 0.9 # Factor indiferente entre 0.1 y 0.9\n",
        "_Q = train_quality_matriz_simple(_T, _R, _board_states, _episodes, _learning_rate, _discount_rate, True)"
      ],
      "metadata": {
        "id": "D1_n0GKAtP3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1614ef-887f-4bf2-cbe2-65cac77700c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estados con recompensas => 1370\n",
            "Filas de Q sin entrenar => 1370\n",
            "Índices no entrenados => []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmo que entrena la matriz Q a raíz de la ecuación de Bellman. El entrenamiento es sólo para el jugador 1 omitiendo los estados del segundo jugador.\n",
        "\n",
        "El factor epsilon-greedy se ha dejado como ecuación de segundo grado para que el 70% prevalezca la fase de exploración y en el último 30% la fase de explotación vaya teniendo más incidencia."
      ],
      "metadata": {
        "id": "7WP_9zdrstRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_quality_matriz_1_player(T, R, board_states, episodes, v, y, info=False):\n",
        "  # Matriz de calidad\n",
        "  Q=np.zeros((T.shape[0],T.shape[1]),dtype=float)\n",
        "\n",
        "  s = 0 # Estado inicial\n",
        "  episode = 0 # Inicialización del entrenamiento\n",
        "\n",
        "  while(episode < episodes):\n",
        "\n",
        "    # Epsilon-Greedy: Selección de acción\n",
        "    epsilon = 1 - (episode/episodes)**2\n",
        "\n",
        "    # Acciones aleatoria al azar número entero en [0,9] que esté permitida\n",
        "    if np.random.rand() < epsilon:\n",
        "      ## Acción aleatoria J1\n",
        "      a = choose_random_action(s, T)\n",
        "      current_s=T[s,a]\n",
        "      if (R[current_s] != 0) or (board_states[current_s].count(0) == 0):\n",
        "        next_s = current_s\n",
        "      else:\n",
        "        # Acción aleatoria J2\n",
        "        next_s=T[current_s, choose_random_action(current_s, T)]\n",
        "\n",
        "    # Acciónes buenas\n",
        "    else:\n",
        "      ## Mejor acción permitida permitida J1\n",
        "      a = choose_best_action(s, T, Q)\n",
        "      current_s=T[s,a]\n",
        "      if (R[current_s] != 0) or (board_states[current_s].count(0) == 0):\n",
        "        next_s = current_s\n",
        "      else:\n",
        "        # Acción de jugador avanzado J2\n",
        "        next_s=T[current_s, choose_advanced_action(current_s, T, R, board_states)]\n",
        "\n",
        "  # Actualización de la matriz de calidad\n",
        "    Qmax = calculate_q_max(next_s, T, Q, True)\n",
        "    Q[s,a] = (1-v) * Q[s,a] + v * (R[next_s] + y*Qmax)\n",
        "\n",
        "    # Cambio de estado\n",
        "    if (R[next_s] != 0) or (board_states[next_s].count(0) == 0):\n",
        "      s = 0\n",
        "    else:\n",
        "      s = next_s\n",
        "\n",
        "    episode+=1\n",
        "\n",
        "  # INFO\n",
        "  if info:\n",
        "    # Comprobación de los estados que no han actualizado su valor en la matriz de calidad y no son finales con recompensa\n",
        "    Rn0 = len(np.array(R)[np.array(R)!=0])\n",
        "    print(\"Estados con recompensas =>\", Rn0)\n",
        "    Qa0 = Q[(Q==0).all(axis=1)].shape[0]\n",
        "    print(\"Filas de Q sin entrenar =>\", Qa0)\n",
        "    #Combiene volver a entrenar si algún estado no ha sido entrenado\n",
        "    print(\"Índices no entrenados =>\", np.setxor1d(np.where((Q == 0).all(axis=1))[0], np.where(np.array(R)!=0)[0]))\n",
        "\n",
        "  return Q\n",
        "\n",
        "_episodes = 200000 # Nº de actualizaciones de la matriz Q\n",
        "_learning_rate = 0.1 # Factor de aprendizaje menor a 1/9 que de mayor peso a la experiencia\n",
        "_discount_rate = 0.9 # Factor indiferente entre 0.1 y 0.9\n",
        "_Q = train_quality_matriz_1_player(_T, _R, _board_states, _episodes, _learning_rate, _discount_rate, True)"
      ],
      "metadata": {
        "id": "h0JoTEJj4uSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6a4887-4edf-4d30-88cb-6349ea3a09f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estados con recompensas => 1370\n",
            "Filas de Q sin entrenar => 3467\n",
            "Índices no entrenados => [   1    2    5 ... 5812 5818 5827]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmo que entrena la matriz Q a raíz de la ecuación de Bellman para ambos jugadores.\n",
        "\n",
        "El factor epsilon-greedy se ha dejado como ecuación de segundo grado para que el 70% prevalezca la fase de exploración y en el último 30% la fase de explotación vaya teniendo más incidencia."
      ],
      "metadata": {
        "id": "C4vT9VGizWgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_quality_matriz_2_players(T, R, board_states, episodes, v, y, info=False):\n",
        "  # Matriz de calidad\n",
        "  Q=np.zeros((T.shape[0],T.shape[1]),dtype=float)\n",
        "  Q1=np.zeros((T.shape[0],T.shape[1]),dtype=float)\n",
        "  Q2=np.zeros((T.shape[0],T.shape[1]),dtype=float)\n",
        "\n",
        "  # INFO\n",
        "  if info:\n",
        "    # Comprobación de los estados que no han actualizado su valor en la matriz de calidad y no son finales con recompensa\n",
        "    Rn0 = len(np.array(R)[np.array(R)!=0])\n",
        "    print(\"Estados con recompensas =>\", Rn0)\n",
        "    Qa0 = Q[(Q==0).all(axis=1)].shape[0]\n",
        "    print(\"Filas de Q sin entrenar al inicio =>\", Qa0)\n",
        "    #Combiene volver a entrenar si algún estado no ha sido entrenado\n",
        "    print(\"Índices no entrenados al inicio =>\", np.setxor1d(np.where((Q == 0).all(axis=1))[0], np.where(np.array(R)!=0)[0]))\n",
        "\n",
        "  episodes = int(episodes/2)\n",
        "\n",
        "  #-------------------------#\n",
        "  # ENTRENAMIENTO JUGADOR 1 #\n",
        "  #-------------------------#\n",
        "\n",
        "  s = 0 # Estado inicial\n",
        "  episode = 0 # Inicialización del entrenamiento\n",
        "\n",
        "  while(episode < episodes):\n",
        "\n",
        "    # Epsilon-Greedy: Selección de acción\n",
        "    epsilon = 1 - (episode/episodes)**2\n",
        "\n",
        "    # Acciones aleatoria al azar número entero en [0,9] que esté permitida\n",
        "    if np.random.rand() < epsilon:\n",
        "      ## Acción aleatoria J1\n",
        "      a = choose_random_action(s, T)\n",
        "      current_s=T[s,a]\n",
        "      if (R[current_s] != 0) or (board_states[current_s].count(0) == 0):\n",
        "        next_s = current_s\n",
        "      else:\n",
        "        # Acción aleatoria J2\n",
        "        next_s=T[current_s, choose_random_action(current_s, T)]\n",
        "\n",
        "    # Acciónes buenas\n",
        "    else:\n",
        "      ## Mejor acción permitida permitida J1\n",
        "      a = choose_best_action(s, T, Q1)\n",
        "      current_s=T[s,a]\n",
        "      if (R[current_s] != 0) or (board_states[current_s].count(0) == 0):\n",
        "        next_s = current_s\n",
        "      else:\n",
        "        # Acción de jugador avanzado J2\n",
        "        next_s=T[current_s, choose_advanced_action(current_s, T, R, board_states)]\n",
        "\n",
        "    # Actualización de Q con Q1\n",
        "    Qmax = calculate_q_max(next_s, T, Q1, True)\n",
        "    Q1[s,a] = (1-v) * Q1[s,a] + v * (R[next_s] + y*Qmax)\n",
        "    Q[s,a] = Q1[s,a]\n",
        "\n",
        "    # Transición de estado\n",
        "    if (R[next_s] != 0) or (board_states[next_s].count(0) == 0):\n",
        "      s = 0\n",
        "    else:\n",
        "      s = next_s\n",
        "\n",
        "    episode+=1\n",
        "\n",
        "  # INFO\n",
        "  if info:\n",
        "    # Comprobación de los estados que no han actualizado su valor en la matriz de calidad y no son finales con recompensa\n",
        "    Qa0 = Q[(Q==0).all(axis=1)].shape[0]\n",
        "    print(\"Filas de Q sin entrenar a mitad =>\", Qa0)\n",
        "    #Combiene volver a entrenar si algún estado no ha sido entrenado\n",
        "    print(\"Índices no entrenados a mitad=>\", np.setxor1d(np.where((Q == 0).all(axis=1))[0], np.where(np.array(R)!=0)[0]))\n",
        "\n",
        "  #-------------------------#\n",
        "  # ENTRENAMIENTO JUGADOR 2 #\n",
        "  #-------------------------#\n",
        "\n",
        "  s = 0 # Estado inicial\n",
        "  episode = 0 # Inicialización del entrenamiento\n",
        "  R = [i*(-1) for i in R]\n",
        "\n",
        "  while(episode < episodes):\n",
        "\n",
        "    # Epsilon-Greedy: Selección de acción\n",
        "    epsilon = 1 - (episode/episodes)**2\n",
        "\n",
        "    # Acciones aleatoria al azar número entero en [0,9] que esté permitida\n",
        "    if np.random.rand() < epsilon:\n",
        "\n",
        "      #Primera acción del jugador 1 en la partida\n",
        "      if s==0:\n",
        "        s=T[s, choose_random_action(s, T)]\n",
        "\n",
        "      ## Acción aleatoria J2\n",
        "      a = choose_random_action(s, T)\n",
        "      current_s=T[s,a]\n",
        "      if (R[current_s] != 0) or (board_states[current_s].count(0) == 0):\n",
        "        next_s = current_s\n",
        "      else:\n",
        "        # Acción aleatoria J1\n",
        "        next_s=T[current_s, choose_random_action(current_s, T)]\n",
        "\n",
        "    # Acciónes buenas\n",
        "    else:\n",
        "      #Primera acción del jugador avanzado en la partida\n",
        "      if s==0:\n",
        "        s=T[s, choose_advanced_action(s, T, R, board_states)]\n",
        "\n",
        "      ## Mejor acción permitida permitida J2\n",
        "      a = choose_best_action(s, T, Q2)\n",
        "      current_s=T[s,a]\n",
        "      if (R[current_s] != 0) or (board_states[current_s].count(0) == 0):\n",
        "        next_s = current_s\n",
        "      else:\n",
        "        # Acción de jugador avanzado J1\n",
        "        next_s=T[current_s, choose_advanced_action(current_s, T, R, board_states)]\n",
        "\n",
        "    # Actualización de Q con Q2\n",
        "    Qmax = calculate_q_max(next_s, T, Q2, True)\n",
        "    Q2[s,a] = (1-v) * Q2[s,a] + v * (R[next_s] + y*Qmax)\n",
        "    Q[s,a] = Q2[s,a]\n",
        "\n",
        "    # Transición de estado\n",
        "    if (R[next_s] != 0) or (board_states[next_s].count(0) == 0):\n",
        "      s = 0\n",
        "    else:\n",
        "      s = next_s\n",
        "\n",
        "    episode+=1\n",
        "\n",
        "  # INFO\n",
        "  if info:\n",
        "    # Comprobación de los estados que no han actualizado su valor en la matriz de calidad y no son finales con recompensa\n",
        "    Qa0 = Q[(Q==0).all(axis=1)].shape[0]\n",
        "    print(\"Filas de Q sin entrenar al final =>\", Qa0)\n",
        "    #Combiene volver a entrenar si algún estado no ha sido entrenado\n",
        "    print(\"Índices no entrenados al final =>\", np.setxor1d(np.where((Q == 0).all(axis=1))[0], np.where(np.array(R)!=0)[0]))\n",
        "\n",
        "  return Q\n",
        "\n",
        "_episodes = 200000 # Nº de actualizaciones de la matriz Q\n",
        "_learning_rate = 0.1 # Factor de aprendizaje menor a 1/9 que de mayor peso a la experiencia\n",
        "_discount_rate = 0.9 # Factor indiferente entre 0.1 y 0.9\n",
        "_Q = train_quality_matriz_2_players(_T, _R, _board_states, _episodes, _learning_rate, _discount_rate, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4fkQaqMzWoi",
        "outputId": "91af7011-6580-4056-eaf6-32d74b6f53ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estados con recompensas => 1370\n",
            "Filas de Q sin entrenar al inicio => 5890\n",
            "Índices no entrenados al inicio => [   0    1    2 ... 5828 5836 5843]\n",
            "Filas de Q sin entrenar a mitad => 3467\n",
            "Índices no entrenados a mitad=> [   1    2    5 ... 5812 5818 5827]\n",
            "Filas de Q sin entrenar al final => 1370\n",
            "Índices no entrenados al final => []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones de comprobación"
      ],
      "metadata": {
        "id": "Sa3sENZnNtd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para ver el tablero y matrices de un estado\n",
        "def check_state(state, T, Q, R, board_states):\n",
        "  df = pd.DataFrame([['x' if i == 1 else 'o' if i == 2 else \"-\" for i in board_states[state]], T[state], Q[state]],\n",
        "                    columns=[\"Acción \"+str(i) for i in list(range(1,10))],\n",
        "                    index=['Tablero', 'T', 'Q'])\n",
        "\n",
        "  df_style = df.style.apply(lambda row: ['color: green' if x == df.loc['Q', :].replace(0,pd.NA).dropna().max(axis=0) else '' for x in row], subset=pd.IndexSlice[['Q'], :])\n",
        "\n",
        "  display(df_style)\n",
        "\n",
        "  print(\"Recompensa:\", R[state])\n",
        "\n",
        "check_state(5827, _T, _Q, _R, _board_states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "9u2wd5bG7Wth",
        "outputId": "a8a24407-e8f9-48af-eaf5-e3ec61ecf554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f5e51028730>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_3997e\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_3997e_level0_col0\" class=\"col_heading level0 col0\" >Acción 1</th>\n",
              "      <th id=\"T_3997e_level0_col1\" class=\"col_heading level0 col1\" >Acción 2</th>\n",
              "      <th id=\"T_3997e_level0_col2\" class=\"col_heading level0 col2\" >Acción 3</th>\n",
              "      <th id=\"T_3997e_level0_col3\" class=\"col_heading level0 col3\" >Acción 4</th>\n",
              "      <th id=\"T_3997e_level0_col4\" class=\"col_heading level0 col4\" >Acción 5</th>\n",
              "      <th id=\"T_3997e_level0_col5\" class=\"col_heading level0 col5\" >Acción 6</th>\n",
              "      <th id=\"T_3997e_level0_col6\" class=\"col_heading level0 col6\" >Acción 7</th>\n",
              "      <th id=\"T_3997e_level0_col7\" class=\"col_heading level0 col7\" >Acción 8</th>\n",
              "      <th id=\"T_3997e_level0_col8\" class=\"col_heading level0 col8\" >Acción 9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_3997e_level0_row0\" class=\"row_heading level0 row0\" >Tablero</th>\n",
              "      <td id=\"T_3997e_row0_col0\" class=\"data row0 col0\" >o</td>\n",
              "      <td id=\"T_3997e_row0_col1\" class=\"data row0 col1\" >o</td>\n",
              "      <td id=\"T_3997e_row0_col2\" class=\"data row0 col2\" >x</td>\n",
              "      <td id=\"T_3997e_row0_col3\" class=\"data row0 col3\" >o</td>\n",
              "      <td id=\"T_3997e_row0_col4\" class=\"data row0 col4\" >x</td>\n",
              "      <td id=\"T_3997e_row0_col5\" class=\"data row0 col5\" >x</td>\n",
              "      <td id=\"T_3997e_row0_col6\" class=\"data row0 col6\" >-</td>\n",
              "      <td id=\"T_3997e_row0_col7\" class=\"data row0 col7\" >x</td>\n",
              "      <td id=\"T_3997e_row0_col8\" class=\"data row0 col8\" >-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3997e_level0_row1\" class=\"row_heading level0 row1\" >T</th>\n",
              "      <td id=\"T_3997e_row1_col0\" class=\"data row1 col0\" >-1</td>\n",
              "      <td id=\"T_3997e_row1_col1\" class=\"data row1 col1\" >-1</td>\n",
              "      <td id=\"T_3997e_row1_col2\" class=\"data row1 col2\" >-1</td>\n",
              "      <td id=\"T_3997e_row1_col3\" class=\"data row1 col3\" >-1</td>\n",
              "      <td id=\"T_3997e_row1_col4\" class=\"data row1 col4\" >-1</td>\n",
              "      <td id=\"T_3997e_row1_col5\" class=\"data row1 col5\" >-1</td>\n",
              "      <td id=\"T_3997e_row1_col6\" class=\"data row1 col6\" >5835</td>\n",
              "      <td id=\"T_3997e_row1_col7\" class=\"data row1 col7\" >-1</td>\n",
              "      <td id=\"T_3997e_row1_col8\" class=\"data row1 col8\" >5828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3997e_level0_row2\" class=\"row_heading level0 row2\" >Q</th>\n",
              "      <td id=\"T_3997e_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
              "      <td id=\"T_3997e_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
              "      <td id=\"T_3997e_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
              "      <td id=\"T_3997e_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
              "      <td id=\"T_3997e_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
              "      <td id=\"T_3997e_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
              "      <td id=\"T_3997e_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
              "      <td id=\"T_3997e_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
              "      <td id=\"T_3997e_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comprobar Estado destino de jugar\n",
        "def check_previous_states(state_checked, T, Q, R, board_states, dict_state_value, dict_value_state):\n",
        "\n",
        "  print(\"Estado:\")\n",
        "  board_state = board_states[state_checked]\n",
        "  board_value = dict_state_value[state_checked]\n",
        "  print(f\"{state_checked} (V {board_value}) => {board_state} => {T[state_checked]} => {Q[state_checked].round(2)} => R {R[state_checked]}\")\n",
        "\n",
        "  coin = 1 if board_state.count(1) - board_state.count(2) == 1 else 2\n",
        "  possible_indexes = np.where(np.array(board_states[state_checked]) == coin)[0]\n",
        "  print(f\"\\nJugadas anteriores -> índices {possible_indexes}\")\n",
        "  for i in possible_indexes:\n",
        "    previous_state = board_value - coin*3**(8-i)\n",
        "    previous_index = dict_value_state[previous_state]\n",
        "    print(f\"{previous_index} (V {previous_state}) => {board_states[previous_index]} => {T[previous_index]} => {Q[previous_index].round(2)}\")\n",
        "\n",
        "check_previous_states(1702, _T, _Q, _R, _board_states, _dict_state_value, _dict_value_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SKmEwMAEXZK",
        "outputId": "19b71ea0-cb0b-471e-f56e-2fcdac61c5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estado:\n",
            "1702 (V 5062) => [0, 2, 0, 2, 2, 1, 1, 1, 1] => [-1 -1 -1 -1 -1 -1 -1 -1 -1] => [0. 0. 0. 0. 0. 0. 0. 0. 0.] => R 10\n",
            "\n",
            "Jugadas anteriores -> índices [5 6 7 8]\n",
            "1698 (V 5035) => [0, 2, 0, 2, 2, 0, 1, 1, 1] => [-1 -1 -1 -1 -1 -1 -1 -1 -1] => [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "1699 (V 5053) => [0, 2, 0, 2, 2, 1, 0, 1, 1] => [3682   -1 1957   -1   -1   -1 1702   -1   -1] => [ 7.85  0.    9.66  0.    0.    0.   10.    0.    0.  ]\n",
            "1700 (V 5059) => [0, 2, 0, 2, 2, 1, 1, 0, 1] => [3684   -1 1959   -1   -1   -1   -1 1702   -1] => [2.93 0.   9.58 0.   0.   0.   0.   5.22 0.  ]\n",
            "1701 (V 5061) => [0, 2, 0, 2, 2, 1, 1, 1, 0] => [3685   -1 1960   -1   -1   -1   -1   -1 1702] => [ 7.47  0.    6.    0.    0.    0.    0.    0.   10.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de Juego"
      ],
      "metadata": {
        "id": "R5f1mr27Pm_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones partida con interfaz"
      ],
      "metadata": {
        "id": "ArjLihQpzCO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tablero de juego con botones 3x3\n",
        "def create_board(on_square_click):\n",
        "  global buttons\n",
        "  buttons =  []\n",
        "  buttons_margin = ['0 0 5px -5px', '0 0 0 0', '0 0 0 5px']\n",
        "  for i in range(9):\n",
        "    button = widgets.Button(description='', button_style='',\n",
        "            layout=widgets.Layout(width= '80px', height= '80px'))\n",
        "    button.layout.margin = buttons_margin[i%3]\n",
        "    button.on_click(on_square_click)\n",
        "    buttons.append(button)\n",
        "\n",
        "    board = widgets.GridBox(buttons,\n",
        "                            layout=widgets.Layout(grid_template_columns=\"repeat(3, 80px)\"))\n",
        "  display(board)\n",
        "\n",
        "#Funcion para modificar el tablero de juego\n",
        "def choose_square(button, description, style):\n",
        "  button.description = description\n",
        "  button.button_style = style\n",
        "  button.disabled = True\n",
        "\n",
        "def disable_squares(buttons):\n",
        "  for button in buttons:\n",
        "    if button.description == '':\n",
        "      button.disabled = True\n",
        "\n",
        "def enable_squares(buttons):\n",
        "  for button in buttons:\n",
        "    if button.description == '':\n",
        "      button.disabled = False\n",
        "\n",
        "def reset_board (buttons):\n",
        "  current_state = 0\n",
        "  for button in buttons:\n",
        "    button.description = ''\n",
        "    button.button_style = \"\"\n",
        "    button.disabled = False"
      ],
      "metadata": {
        "id": "yXK3nVM09qjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función que comprueba estado final\n",
        "def finish_turn(player, button):\n",
        "  global current_state\n",
        "  global board_states\n",
        "  global R\n",
        "  global rewards\n",
        "\n",
        "  if player == 1:\n",
        "    print(\"Tú: \", current_state)\n",
        "  else:\n",
        "    print(\"IA: \", current_state)\n",
        "\n",
        "  # Se marca la casilla elegida\n",
        "  coin = 'O' if player == 1 else 'X' # \"Jugador 'O' - IA 'X\"\n",
        "  # Jugado con victoria de un jugador\n",
        "  if R[current_state] == rewards[0] or R[current_state] == rewards[1]:\n",
        "    style = 'success' if player == 1 else 'danger' # \"Jugador 'verde' - IA 'rojo\"\n",
        "  # Jugado sin victoria\n",
        "  else:\n",
        "    style = 'info' if player == 1 else 'warning' # \"Jugador 'azul' - IA 'naranja\"\n",
        "  choose_square(button, coin, style)\n",
        "\n",
        "  # Turno final\n",
        "  if (R[current_state] != 0) or (board_states[current_state].count(0) == 0):\n",
        "    disable_squares(buttons)\n",
        "    if R[current_state] == rewards[0]:\n",
        "      print(\"Jugador 1 gana\")\n",
        "    elif R[current_state] == rewards[1]:\n",
        "      print(\"Jugador 2 gana\")\n",
        "    else:\n",
        "      print(\"Empate\")\n",
        "  #Turno normal\n",
        "  else:\n",
        "    if player == 1:\n",
        "      disable_squares(buttons)\n",
        "      launch_ia()\n",
        "    else:\n",
        "      enable_squares(buttons)\n",
        "\n",
        "# Función para manejar el clic en el botón\n",
        "def push_square(button):\n",
        "  global current_state\n",
        "  global buttons\n",
        "  indice = buttons.index(button)\n",
        "  current_state = T[current_state, indice]\n",
        "  finish_turn(1, button)\n",
        "\n",
        "# Función para que la IA juegue\n",
        "def launch_ia():\n",
        "  global current_state\n",
        "  global buttons\n",
        "  # Obtener botones disponibles\n",
        "  buttons_availables = [button for button in buttons if button.description == '']\n",
        "  if buttons_availables:\n",
        "    if (board_states[current_state].count(1) - board_states[current_state].count(2)) == 0:\n",
        "      index = choose_best_action(current_state, T, Q)\n",
        "    else:\n",
        "      index = choose_best_action(current_state, T, Q)\n",
        "    current_state = T[current_state, index]\n",
        "    button = buttons[index]\n",
        "    finish_turn(2, button)"
      ],
      "metadata": {
        "id": "yLmE0R1bHRj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones de partidas automáticas"
      ],
      "metadata": {
        "id": "C8BccEG1TfkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para que mueva la IA\n",
        "def move_IA(current_state, T, Q):\n",
        "  if (board_states[current_state].count(1) - board_states[current_state].count(2)) == 0:\n",
        "    index = choose_best_action(current_state, T, Q)\n",
        "  else:\n",
        "    index = choose_best_action(current_state, T, Q)\n",
        "  current_state = T[current_state, index]\n",
        "\n",
        "  return current_state\n",
        "# Función para movimiento aleatorio\n",
        "def move_random(current_state, T):\n",
        "  index = choose_random_action(current_state, T)\n",
        "  current_state = T[current_state, index]\n",
        "  return current_state\n",
        "# Función para movimiento avanzado\n",
        "def move_advanced(current_state, T):\n",
        "  global R\n",
        "  global board_states\n",
        "  index = choose_advanced_action(current_state, T, R, board_states)\n",
        "  current_state = T[current_state, index]\n",
        "  return current_state\n",
        "\n",
        "#Algoritmo de partida automático donde en cada turno mueve un jugador\n",
        "def start_ia_vs_rival(T,Q, IA_play_first=True, is_rival_random=True):\n",
        "\n",
        "  n_wins_j1 = 0\n",
        "  n_wins_j2 = 0\n",
        "  n_draws = 0\n",
        "\n",
        "  i_aux = 0 if IA_play_first else 1\n",
        "\n",
        "  for i in range(10000):\n",
        "\n",
        "    current_state = 0\n",
        "\n",
        "    while R[current_state] == 0 and board_states[current_state].count(0) != 0:\n",
        "      if (i+i_aux)%2 == 0:\n",
        "        current_state = move_IA(current_state, T, Q)\n",
        "      else:\n",
        "        if is_rival_random:\n",
        "          current_state = move_random(current_state, T)\n",
        "        else:\n",
        "          current_state = move_advanced(current_state, T)\n",
        "\n",
        "    if R[current_state] == rewards[0]:\n",
        "      n_wins_j1+=1\n",
        "    if R[current_state] == rewards[1]:\n",
        "      n_wins_j2+=1\n",
        "    else:\n",
        "      n_draws+=1\n",
        "\n",
        "  print(\"Victorias J1: \",n_wins_j1 )\n",
        "  print(\"Victorias J2: \",n_wins_j2 )\n",
        "  print(\"Empates:      \",n_draws )"
      ],
      "metadata": {
        "id": "48OZnhBt0zxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Juego"
      ],
      "metadata": {
        "id": "E-NzJeq_zF3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables e hiperparámetros"
      ],
      "metadata": {
        "id": "mFPoFFc2Wp-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompensas de victoria/derrota/empate\n",
        "rewards = [10, -10, 1]\n",
        "n_episodes = 300000 #epidios de entrenamiento\n",
        "v = 0.1 #Factor de aprendizaje menor a 1/9 que de mayor peso a la experiencia\n",
        "y = 0.7 #Factor de descuento indiferente entre 0.1 y 0.9\n",
        "\n",
        "board_states, R = create_board_and_rewards_vectors(rewards, False)\n",
        "dict_state_value, dict_value_state = create_dicts_state_value(board_states, False)\n",
        "T = create_transition_matrix(board_states, R, dict_state_value, dict_value_state, False)\n",
        "\n",
        "#Elegir algoritmo de entrenamiento\n",
        "#Q = train_quality_matriz_simple(T, R, board_states, n_episodes, v, y, True)\n",
        "#Q = train_quality_matriz_1_player(T, R, board_states, n_episodes, v, y, True)\n",
        "Q = train_quality_matriz_2_players(T, R, board_states, n_episodes, v, y, True)"
      ],
      "metadata": {
        "id": "xS-Lt4cP5etL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d29502-cda3-42a8-8a64-c5d55e79d6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estados con recompensas => 1370\n",
            "Filas de Q sin entrenar al inicio => 5890\n",
            "Índices no entrenados al inicio => [   0    1    2 ... 5828 5836 5843]\n",
            "Filas de Q sin entrenar a mitad => 3467\n",
            "Índices no entrenados a mitad=> [   1    2    5 ... 5812 5818 5827]\n",
            "Filas de Q sin entrenar al final => 1370\n",
            "Índices no entrenados al final => []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tablero"
      ],
      "metadata": {
        "id": "IGCKwP2cW1ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interfaz para jugar partidas"
      ],
      "metadata": {
        "id": "QyY3r_kPDHY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IA_player1 = True\n",
        "\n",
        "#Variable de estado\n",
        "current_state = 0\n",
        "buttons = []\n",
        "# Mostrar el tablero\n",
        "create_board(push_square)\n",
        "\n",
        "if IA_player1:\n",
        "  launch_ia()"
      ],
      "metadata": {
        "id": "d7hO-ZvhyGNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partidas automáticas\n",
        "\n"
      ],
      "metadata": {
        "id": "lPkIlQuwQ96_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estadísticas de victorias contra ciertos rivales."
      ],
      "metadata": {
        "id": "P3KLwTzQDPNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IA vs jugador random\n",
        "start_ia_vs_rival(T,Q, IA_play_first=True, is_rival_random=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZrLY_IV2jqD",
        "outputId": "f56cb9f1-6da0-4a84-dcec-e56d5bb728c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Victorias J1:  7971\n",
            "Victorias J2:  1381\n",
            "Empates:       8619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IA vs jugador avanzado\n",
        "start_ia_vs_rival(T,Q, IA_play_first=True, is_rival_random=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IelJb9siBzda",
        "outputId": "b978c3de-8a80-4bd8-b140-864be416eb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Victorias J1:  5000\n",
            "Victorias J2:  301\n",
            "Empates:       9699\n"
          ]
        }
      ]
    }
  ]
}